{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lo2qtW6twOmZ"
   },
   "source": [
    "# Устанавливаем все библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "InVRokr6vUAQ"
   },
   "outputs": [],
   "source": [
    "### Installing dependecies\n",
    "%matplotlib inline\n",
    "!pip install Box2D  > /dev/null 2>&1\n",
    "!pip install gym[all] pyvirtualdisplay > /dev/null 2>&1\n",
    "!pip install tqdm > /dev/null 2>&1          \n",
    "!apt update > /dev/null 2>&1\n",
    "!apt install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install gym-super-mario-bros==7.3.0 > /dev/null 2>&1\n",
    "\n",
    "### For logger's plots parsing\n",
    "!sudo python3 -m pip install wheel > /dev/null 2>&1\n",
    "!sudo python3 -m pip install pandas > /dev/null 2>&1\n",
    "!sudo python3 -m pip install seaborn > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehAVX5V6wY9B"
   },
   "source": [
    "# Подключаем библиотеки (все импорты писать сюда)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "iUNAIuYtviyY"
   },
   "outputs": [],
   "source": [
    "### Includes libs\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy, itertools\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# Visualization\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "# Logging\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRCg4GuFwxdo"
   },
   "source": [
    "# Переносим вычисления на GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LNHCvVMwvo6D",
    "outputId": "1877a7c9-2775-4d55-9722-879d22b73565"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вы используете gpu, все ок\n"
     ]
    }
   ],
   "source": [
    "### Connect to GPU\n",
    "# Устройство, на котором будет работать PyTorch.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # GPU trains faster\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Вы используете gpu, все ок\")\n",
    "else:\n",
    "    print(\"Вы не используете gpu!! Если вы в google colab, выберите gpu в Runtime -> Change Runtime type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UeDAC9qw3ua"
   },
   "source": [
    "# Инициализируем окружение игры и ограничиваем действия Марио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "wmWlbPKiv7KT",
    "outputId": "2fe93a14-022a-42d9-bfac-de6da61026fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x240 at 0x7F0DBE3F9160>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create environment\n",
    "# Инициализация среды\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"], [\"right\", \"A\", \"B\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")\n",
    "\n",
    "# как сейчас выглядит состояние\n",
    "Image.fromarray(next_state, \"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ4gv-lcw-0n"
   },
   "source": [
    "# Преобразуем окружение, чтобы нейронка лучше обучалась\n",
    "- `SkipFrame` - Пропускаем часть фреймов из игры (считываем каждый `skip`-ый фрейм) \n",
    "- `GrayScaleObservation` - преобразуем картинку в черно-белую\n",
    "- `ResizeObservation` - сжимаем изображение shape: `(4, 84, 84)`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "XuO-FNmCwEBx"
   },
   "outputs": [],
   "source": [
    "### Transform environment\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "\n",
    "        # [H, W, C] = [0, 1, 2]\n",
    "        # [C, H, W] = [2, 0, 1]\n",
    "        # используйте np.transpose чтобы перевернуть оси в массиве [H, W, C] -> [C, H, W] (H=hight, W=width, C=channels)\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)  # TODO примените метод permute_orientation к observation\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXpKmNHgyInQ"
   },
   "source": [
    "# Задаем константы (параметры обучения и прочая дичь)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "vZ6H4ehnyNvJ"
   },
   "outputs": [],
   "source": [
    "### constants\n",
    "# INPUT\n",
    "\n",
    "STATE_SHAPE = (4, 84, 84) #int(np.prod(env.observation_space.shape)) # 8 #  # dimensions of input image\n",
    "ACTION_SHAPE = env.action_space.n # number of actions\n",
    "\n",
    "# TRAINING\n",
    "TOTAL_EPISODES = 50000\n",
    "GAMMA = 0.999\n",
    "LEARNING_RATE = 0.00025\n",
    "EPS_MAX = 1.0\n",
    "EPS_MIN = 0.1\n",
    "EPS_DECAY = 0.50 * TOTAL_EPISODES # eps will become smaller earlier in the learning process\n",
    "LEARN_FREQ = 3 # after how many steps to learn\n",
    "LOG_FREQ = 40\n",
    "PLOT_LOG_FREQ = 1000 \n",
    "\n",
    "UPDATE_TARGET_FREQ = 200 # hard update target net with online net\n",
    "\n",
    "# REPLAY BUFFER\n",
    "BUFFER_SIZE = 100000 # max size of buffer\n",
    "MIN_BUFFER_SIZE = 1000 # at least put 1000 transitions in a buffer before learning\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# SAVING\n",
    "SAVE_DIR = Path(\"saver_surgery\") # where checkpoints saved\n",
    "if not Path('saver_surgery').is_dir():\n",
    "    SAVE_DIR.mkdir(parents=True)\n",
    "SAVE_FREQ = 500\n",
    "\n",
    "# VISUALIZATION\n",
    "VIDEOS_DIR = Path(\"videos_surgery\") # where checkpoints saved\n",
    "if not Path('videos_surgery').is_dir():\n",
    "    VIDEOS_DIR.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VV4fMiV0ksy"
   },
   "source": [
    "# Визуализация\n",
    "\n",
    "```\n",
    "visualize(agent, episodes=1, action_limit=None, fileName=None)\n",
    "```\n",
    "\n",
    "`agent` - our Mario\n",
    "\n",
    "`episodes` - number of videos\n",
    "\n",
    "`action_limit` - limit of glazed actions\n",
    "\n",
    "`fileName` - name of the file to upload gif video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "Bnx7eZrv0puz"
   },
   "outputs": [],
   "source": [
    "### Visualization\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "\n",
    "animation.embed_limit = 2**800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "z8ts9Zz80qpw"
   },
   "outputs": [],
   "source": [
    "def init_figure():\n",
    "    fig, ax = plt.subplots(figsize=(8, 6), nrows=1, ncols=1)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    plt.tight_layout()\n",
    "    return fig, ax\n",
    "\n",
    "def show_video(frames, fileName=None):\n",
    "    fig, ax = init_figure()\n",
    "    ims = []\n",
    "    for f in frames:\n",
    "        im = plt.imshow(f, animated=True)\n",
    "        ims.append([im])\n",
    "\n",
    "    # creating animation\n",
    "    anim = animation.ArtistAnimation(fig, ims, interval = 40, blit=True)\n",
    "    \n",
    "    # saving gif file on drive\n",
    "    if fileName != None:\n",
    "        writergif = animation.PillowWriter(fps=30) \n",
    "        anim.save(fileName, writergif)\n",
    "        print(\"Video successfully saved!\")\n",
    "    \n",
    "    ipythondisplay.display(HTML(anim.to_jshtml()))\n",
    "    plt.close()\n",
    "\n",
    "# action_limit == None => non limit\n",
    "# fileName - name of the file to upload\n",
    "def visualize(agent, episodes=1, action_limit=None, fileName=None):\n",
    "    \n",
    "    bare_env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "    bare_env = JoypadSpace(bare_env, [[\"right\"], [\"right\", \"A\"], [\"right\", \"A\", \"B\"]])\n",
    "\n",
    "    for e in range(episodes):\n",
    "        frames = []\n",
    "        \n",
    "        state = env.reset()\n",
    "        bare_state = bare_env.reset()\n",
    "        \n",
    "        done = False\n",
    "        cnt = 0\n",
    "        while not done:\n",
    "            cnt += 1\n",
    "            if (action_limit != None) and (cnt > action_limit):\n",
    "                break\n",
    "                \n",
    "            frames.append(Image.fromarray(bare_state, \"RGB\"))\n",
    "            \n",
    "            action = agent.act(state, episode=None, explore=False)\n",
    "            \n",
    "            for i in range(4):\n",
    "                bare_state, _, _done, info = bare_env.step(action)\n",
    "                \n",
    "                if _done or info['flag_get']:\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            if done: # хз, но это нужно\n",
    "                break\n",
    "            \n",
    "            state, _, done, info = env.step(action)\n",
    "            \n",
    "            if info['flag_get']:\n",
    "                done = True\n",
    "        \n",
    "        show_video(frames, fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PuqYJpzE014U"
   },
   "source": [
    "# Класс нейронки (ее структура прописана тут)\n",
    "- `model` - поле с инициализированной модель сети\n",
    "- `forward` - скормливаем сети входные данные\n",
    "- `get_action` - возвращает действие, на основании значений функции `Q` (Не Epsilon-Greedy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "smMX_Gkf0_Be"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "      c, h, w = STATE_SHAPE\n",
    "\n",
    "      if c != 4:\n",
    "        raise ValueError(f\"Expecting input channels: 4, got: {c}\")\n",
    "      if h != 84:\n",
    "        raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "      if w != 84:\n",
    "        raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "      \"\"\"\n",
    "      В нейронной сети нужно начать с конволюционных слоев (CNN) для обработки картинок, после которых идут полносвязные (FC).\n",
    "      Между слоями функция активации (ReLU).\n",
    "      Дефолтная архитектура: input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "      Дефолтные параметры:\n",
    "      CNN1: (in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "      CNN2: (in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "      CNN3: (in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "      Linear 1: (3136, 512)\n",
    "      Linear 2: (512, out_dim)\n",
    "      От этой архитектуры можно отклоняться (на свой страх и риск) Сделать поменьше - быстрее обучится. \n",
    "      Сделать побольше - потенциально точнее, но времени на это нет.\n",
    "      Можно попробовать Dropout слой между линейными.\n",
    "      Можно попробовать добавить l2 регуляризацию - это в оптимизаторе.\n",
    "      \"\"\"\n",
    "      self.model = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "          nn.ReLU(),\n",
    "\n",
    "          nn.Flatten(),\n",
    "\n",
    "          nn.Linear(3136, 512),\n",
    "          nn.ReLU(),\n",
    "\n",
    "          nn.Linear(512, ACTION_SHAPE),\n",
    "      )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "  # selecting an action in Q-learning algorithm\n",
    "  def get_action(self, state):\n",
    "    # converting state (obs) to tenser array\n",
    "    state_t = torch.as_tensor(np.asarray(state), dtype=torch.float32, device=device)\n",
    "    q_values = self(state_t.unsqueeze(0))\n",
    "\n",
    "    # getting index of max Q-value\n",
    "    max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "    action = max_q_index.detach().item()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfdQsvTX3I2N"
   },
   "source": [
    "# Класс самого алгоритма DQN\n",
    "- `online_model` - обучаемая модель \n",
    "- `target_model` - вспомогательная модель\n",
    "- `act` - EpsilonGreedy реализация (запрашиваем жадный `action` у нейронки, либо берем рандомный `action`)\n",
    "- `compute_loss` - вычисляем `loss`-функцию (`mse` в нашем случаи)\n",
    "- `learn` - обновляем веса в `online_net` и копируем ее в `target_net`, когда нужно\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "lPq3l-ND3N70"
   },
   "outputs": [],
   "source": [
    "### DQN ###\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Init online and target net\n",
    "        self.online_model = Network()\n",
    "        self.target_model = Network()\n",
    "        # Update target net\n",
    "        self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "\n",
    "        # Transport calculation to GPU\n",
    "        self.online_model.to(device) \n",
    "        self.target_model.to(device)\n",
    "\n",
    "        self.eps = EPS_MAX\n",
    "        self.steps = 0 # only for learn function\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.online_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    def act(self, state, episode, explore=True):\n",
    "        self.eps = np.interp(episode, [0, EPS_DECAY], [EPS_MAX, EPS_MIN])\n",
    "\n",
    "        if (explore == True and random.random() < self.eps):\n",
    "            return random.randint(0, ACTION_SHAPE - 1)\n",
    "        else:\n",
    "            return self.online_model.get_action(state)\n",
    "\n",
    "\n",
    "    # returns loss value \n",
    "    def compute_loss(self, batch):\n",
    "        state, action, next_state, reward, done = batch\n",
    "\n",
    "        state = torch.as_tensor(np.asarray(state), dtype=torch.float32, device=device)\n",
    "        next_state = torch.as_tensor(np.asarray(next_state), dtype=torch.float32, device=device)\n",
    "        reward = torch.as_tensor(np.asarray(reward), dtype=torch.float32, device=device).unsqueeze(-1)\n",
    "        done = torch.as_tensor(np.asarray(done), dtype=torch.int64, device=device).unsqueeze(-1)\n",
    "        action = torch.as_tensor(np.asarray(action), dtype=torch.int64, device=device).unsqueeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q = self.target_model(next_state).max(dim=1, keepdim=True)[0] # batch_size\n",
    "            target_q = reward + GAMMA * (1 - done) * target_q\n",
    "        \n",
    "        q = self.online_model(state)\n",
    "\n",
    "        q = torch.gather(input=q, dim=1, index=action) # batch_size\n",
    "        loss = nn.functional.mse_loss(q, target_q)\n",
    "        return loss\n",
    "\n",
    "    def learn(self, batch):\n",
    "        self.steps += 1\n",
    "\n",
    "        if self.steps % LEARN_FREQ != 0:\n",
    "            return None\n",
    "\n",
    "        loss = self.compute_loss(batch)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # HARD UPDATE\n",
    "        if self.steps % UPDATE_TARGET_FREQ == 0: \n",
    "            self.target_model.load_state_dict(self.online_model.state_dict())\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-bNGOQM8zZP"
   },
   "source": [
    "# ReplayBuffer - Класс буфера реплея\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "WqPe1Xln86iQ"
   },
   "outputs": [],
   "source": [
    "### Replay Buffer ###\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, size=BUFFER_SIZE):\n",
    "        self.data = deque(maxlen=size)\n",
    "        self.max_size = BUFFER_SIZE\n",
    "    \n",
    "    def add(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def sample(self, size=BATCH_SIZE):\n",
    "        batch = random.sample(self.data, size)\n",
    "        return list(zip(*batch))\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def fill_with_random(self, env):\n",
    "      state = env.reset()\n",
    "      for _ in range(MIN_BUFFER_SIZE):\n",
    "        # getting random action from env\n",
    "        action = env.action_space.sample()\n",
    "        # next_obs == next_state, rew == reward\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        # transion that will be added to the replay buffer\n",
    "        transition = (state, action, next_state, reward, done)\n",
    "        self.data.append(transition)\n",
    "        # updating state to a new state (obs -> next_obs)\n",
    "        state = next_state\n",
    "        # if the current state is terminal, reset the env \n",
    "        if (done):\n",
    "          state = env.reset()\n",
    "    \n",
    "    def fill_with_eps_greedy(self, dqn, episode):\n",
    "        state = env.reset()\n",
    "        for _ in range(MIN_BUFFER_SIZE):\n",
    "            # getting epsilon greedy action\n",
    "            action = dqn.act(state, episode)\n",
    "            # next_obs == next_state, rew == reward\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # transion that will be added to the replay buffer\n",
    "            transition = (state, action, next_state, reward, done)\n",
    "            self.data.append(transition)\n",
    "            # updating state to a new state (obs -> next_obs)\n",
    "            state = next_state\n",
    "            # if the current state is terminal, reset the env \n",
    "            if (done):\n",
    "              state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMJLRiTOSxXN"
   },
   "source": [
    "# Logger\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "save_dir = Path(\"dir\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "```\n",
    "\n",
    "`save_dir` - directory to save logging files\n",
    "\n",
    "`logger.log_step(reward, loss, q)` - logging data inside the class after each step of a model in the `env`\n",
    "\n",
    "`logger.log_episode()` - (fire after each episode ends) logging data after episode ends\n",
    "\n",
    "`logger.record(episode, epsilon)` - prints data to the console and logs it to the file named `log` in specified folder, creates plots of `reward`, `loss`, `average of Q-value function`, `duration of episode`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "e0W1g0j8S0US"
   },
   "outputs": [],
   "source": [
    "### Metric Logger\n",
    "from os import path\n",
    "import csv\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, _dir):\n",
    "        self.save_dir = Path(_dir)\n",
    "        if not Path(_dir).is_dir():\n",
    "            self.save_dir.mkdir(parents=True)\n",
    "        \n",
    "        self.save_log = self.save_dir / \"log.csv\"\n",
    "        \n",
    "        if (path.isfile(self.save_log) == False):\n",
    "            headers = ['Episode', 'Epsilon', 'MeanReward', 'MeanLoss', 'TimeDelta', 'Time']\n",
    "            with open(self.save_log, \"a\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow(headers)\n",
    "\n",
    "        # History metrics\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "        \n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "        \n",
    "        # paths to the save_dir folder + file's names\n",
    "        self.rewards_plot = self.save_dir / \"reward_plot.jpg\"\n",
    "        self.losses_plot  = self.save_dir / \"loss_plot.jpg\"\n",
    "    \n",
    "    \n",
    "    # appends total reward and loss after each episode\n",
    "    def append(self, reward, loss):\n",
    "        self.rewards.append(reward)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "    # adding data to a file\n",
    "    def record(self, episode, epsilon):\n",
    "        mean_reward = np.round(np.mean(self.rewards[-100:]), 3)\n",
    "        mean_loss = np.round(np.mean(self.losses[-100:]), 3)\n",
    "        \n",
    "        # counting time between prev and cur records - time delta\n",
    "        prev_record_time = self.record_time\n",
    "        self.record_time  = time.time()\n",
    "        duration = np.round(self.record_time - prev_record_time, 3)\n",
    "        \n",
    "        # printing data\n",
    "        print(\n",
    "            f\"Episode: {episode}    \"\n",
    "            f\"Epsilon: {epsilon}    \"\n",
    "            f\"Mean Reward: {mean_reward}    \"\n",
    "            f\"Mean Loss: {mean_loss}    \"\n",
    "            f\"Time Delta: {duration}    \"\n",
    "            f\"Time: {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "        \n",
    "        # adding data to a saving file\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            fields = [episode, epsilon, mean_reward, mean_loss, duration, datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')]\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(fields)\n",
    "            \n",
    "    # plotting\n",
    "    def create_plots(self):\n",
    "        df = pd.read_csv(logger.save_log)\n",
    "        Epsilon_plot = sns.lineplot(x=df.Episode, y=df.Epsilon).get_figure()\n",
    "        Epsilon_plot.savefig(f\"{self.save_dir}/Epsilon_plot.png\")\n",
    "        plt.clf()\n",
    "        MeanReward_plot = sns.lineplot(x=df.Episode, y=df.MeanReward).get_figure()\n",
    "        MeanReward_plot.savefig(f\"{self.save_dir}/MeanReward_plot.png\")\n",
    "        plt.clf()\n",
    "        MeanLoss_plot = sns.lineplot(x=df.Episode, y=df.MeanLoss).get_figure()\n",
    "        MeanLoss_plot.savefig(f\"{self.save_dir}/MeanLoss_plot.png\")\n",
    "        plt.clf()\n",
    "        TimeDelta_plot = sns.lineplot(x=df.Episode, y=df.TimeDelta).get_figure()\n",
    "        TimeDelta_plot.savefig(f\"{self.save_dir}/TimeDelta_plot.png\")\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of logger functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = MetricLogger(\"logger\")\n",
    "\n",
    "# for e in range(1000):\n",
    "#     done = bool(random.randint(0, 10) % 2)\n",
    "#     total_reward = 0.0\n",
    "#     loss_sum = 0.0\n",
    "#     steps = 0\n",
    "    \n",
    "#     mean_loss = 0.0\n",
    "    \n",
    "#     while not done:\n",
    "#         steps          += 1\n",
    "#         loss_sum       += random.randint(0, 100)\n",
    "#         total_reward   += random.randint(0, 100)\n",
    "        \n",
    "#         if(random.random() > 0.7):\n",
    "#             done = True\n",
    "    \n",
    "#     if(steps != 0):\n",
    "#         mean_loss = loss_sum / steps\n",
    "        \n",
    "#     logger.append(total_reward, mean_loss)\n",
    "    \n",
    "#     if(e % 20 == 0):\n",
    "#         logger.record(e, random.random())\n",
    "        \n",
    "# logger.create_plots()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fe5jmx2_2WsW"
   },
   "source": [
    "# Saver (saves DQN)\n",
    "\n",
    "`serialize(dqn, episode)` - serializing dqn in file with the new index, `dqn.step` is equal to `episode`\n",
    "\n",
    "`deserialize(index=-1)` - bringing last update of the DQN by default from serialization folder (but can be any index that is not out of the folder's range, size of folder can be brought by `get_size()` method, you can use indexes smaller than 0 too), returns DQN and Replay buffer\n",
    "\n",
    "`get_size()` - returns amount of updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "b0OKPOtL2sKD"
   },
   "outputs": [],
   "source": [
    "class Saver:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_size(self):\n",
    "        cur_size = 0\n",
    "        for base, dirs, files in os.walk(SAVE_DIR):\n",
    "          for file in files:\n",
    "            cur_size += 1\n",
    "        return cur_size\n",
    "\n",
    "    def serialize(self, dqn, episode):\n",
    "        # filePath = f\"{SAVE_DIR}/{datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")}.pt\"\n",
    "\n",
    "        # getting folder info\n",
    "        cur_size = self.get_size()\n",
    "\n",
    "        # creating file name\n",
    "        filePath = f\"{SAVE_DIR}/{cur_size}.pt\"\n",
    "\n",
    "        # saving DQN in the folder\n",
    "        dqn.step = episode\n",
    "        torch.save(dqn, filePath)\n",
    "\n",
    "    def deserialize(self, index=-1): # -> dqn, replay_buffer\n",
    "\n",
    "        # getting file index\n",
    "        cur_size = self.get_size()\n",
    "        if (index < 0):\n",
    "          index = cur_size+index\n",
    "        if (index < 0) or (index >= cur_size):\n",
    "          print(\"Error: index out of range\")\n",
    "          return None, None\n",
    "\n",
    "        # receiving DQN\n",
    "        filePath = f\"{SAVE_DIR}/{index}.pt\"\n",
    "        dqn = torch.load(filePath)\n",
    "\n",
    "        # creating replay buffer\n",
    "        replay_buffer = ReplayBuffer()\n",
    "        replay_buffer.fill_with_eps_greedy(dqn=dqn, episode=dqn.step)\n",
    "        return dqn, replay_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of serializer functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "lmLD9OkC5fi_"
   },
   "outputs": [],
   "source": [
    "# dqn = DQN()\n",
    "# sv = Saver()\n",
    "# sv.serialize(dqn, 0)\n",
    "# sv.serialize(dqn, 2)\n",
    "# sv.serialize(dqn, 5)\n",
    "# sv.serialize(dqn, 10)\n",
    "\n",
    "# print(sv.get_size())\n",
    "# dqn, replay_buffer = sv.deserialize()\n",
    "# print(sv.get_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq6Jhmft8qNr"
   },
   "source": [
    "# Главный цикл тренеровки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "Fha5vnau8s49"
   },
   "outputs": [],
   "source": [
    "def train(dqn,\n",
    "          env,\n",
    "          replay_buffer,\n",
    "          logger,\n",
    "          saver,\n",
    "          start_episode=0,\n",
    "          total_episodes=TOTAL_EPISODES\n",
    "          ):\n",
    "\n",
    "  \n",
    "\n",
    "  #rng = tqdm(range(total_episodes))\n",
    "  for episode in range(start_episode, total_episodes):\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    \n",
    "    episode_reward = 0.0\n",
    "    loss_sum = 0.0\n",
    "    mean_loss = 0.0\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # Count steps in the episode\n",
    "        step += 1\n",
    "        \n",
    "\n",
    "        # Take action with Epsilon-Greedy Policy\n",
    "        action = dqn.act(state, episode=episode)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # summing reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # Save transition to replay buffer\n",
    "        replay_buffer.add((state, action, next_state, reward, done))\n",
    "        \n",
    "        # Get next state\n",
    "        state = next_state  \n",
    "\n",
    "        # Start updating online_net weights\n",
    "        batch = replay_buffer.sample()\n",
    "        loss = dqn.learn(batch) # loss might be None (because we dont learn every step)\n",
    "\n",
    "        # summing loss\n",
    "        if (loss != None):\n",
    "            loss_sum += loss\n",
    "\n",
    "        # Check if end of game (end of episode)\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "        \n",
    "    if(step != 0):\n",
    "        mean_loss = loss_sum / step\n",
    "        \n",
    "    logger.append(episode_reward, mean_loss)\n",
    "    \n",
    "    if episode % LOG_FREQ == 0:\n",
    "        logger.record(episode, dqn.eps)\n",
    "    if episode % PLOT_LOG_FREQ == 0:\n",
    "        logger.create_plots()\n",
    "\n",
    "    # Saving\n",
    "    if (episode % SAVE_FREQ == 0):\n",
    "      saver.serialize(dqn, episode)\n",
    "      pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-ZbcTHXB_8q"
   },
   "source": [
    "# Запускаем Марио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LMP_AZT1CEf5",
    "outputId": "4242a0f0-a19c-4f7e-c97b-61dbc39931d7"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# # Create agent\n",
    "# mario = DQN()\n",
    "\n",
    "# # Create saver\n",
    "# saver = Saver()\n",
    "\n",
    "# # Init replay_buffer\n",
    "# replay_buffer = ReplayBuffer()\n",
    "# replay_buffer.fill_with_random(env)\n",
    "\n",
    "# #Init logger\n",
    "# logger = MetricLogger(\"logger\")\n",
    "\n",
    "# # Start training\n",
    "# train(\n",
    "#     dqn=mario,\n",
    "#     env=env,\n",
    "#     replay_buffer=replay_buffer,\n",
    "#     logger=logger,\n",
    "#     saver=saver\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lywGE9nLCpV2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20000    Epsilon: 0.28    Mean Reward: 233.0    Mean Loss: 633.998    Time Delta: 0.749    Time: 2021-05-09T13:27:46\n",
      "Episode: 20040    Epsilon: 0.27856000000000003    Mean Reward: 858.195    Mean Loss: 297.481    Time Delta: 117.755    Time: 2021-05-09T13:29:44\n",
      "Episode: 20080    Epsilon: 0.27712000000000003    Mean Reward: 929.642    Mean Loss: 356.556    Time Delta: 135.196    Time: 2021-05-09T13:31:59\n",
      "Episode: 20120    Epsilon: 0.27568000000000004    Mean Reward: 977.16    Mean Loss: 420.925    Time Delta: 146.642    Time: 2021-05-09T13:34:26\n",
      "Episode: 20160    Epsilon: 0.27423999999999993    Mean Reward: 1042.76    Mean Loss: 511.42    Time Delta: 164.737    Time: 2021-05-09T13:37:11\n",
      "Episode: 20200    Epsilon: 0.27279999999999993    Mean Reward: 1057.21    Mean Loss: 613.807    Time Delta: 137.532    Time: 2021-05-09T13:39:28\n",
      "Episode: 20240    Epsilon: 0.27135999999999993    Mean Reward: 1031.78    Mean Loss: 739.587    Time Delta: 141.543    Time: 2021-05-09T13:41:50\n",
      "Episode: 20280    Epsilon: 0.26991999999999994    Mean Reward: 974.86    Mean Loss: 828.606    Time Delta: 124.063    Time: 2021-05-09T13:43:54\n",
      "Episode: 20320    Epsilon: 0.26847999999999994    Mean Reward: 964.95    Mean Loss: 900.414    Time Delta: 125.706    Time: 2021-05-09T13:46:00\n",
      "Episode: 20360    Epsilon: 0.26703999999999994    Mean Reward: 965.74    Mean Loss: 996.521    Time Delta: 131.705    Time: 2021-05-09T13:48:11\n",
      "Episode: 20400    Epsilon: 0.26559999999999995    Mean Reward: 950.35    Mean Loss: 1127.032    Time Delta: 118.859    Time: 2021-05-09T13:50:10\n",
      "Episode: 20440    Epsilon: 0.26415999999999995    Mean Reward: 925.59    Mean Loss: 1298.356    Time Delta: 146.921    Time: 2021-05-09T13:52:37\n",
      "Episode: 20480    Epsilon: 0.26271999999999995    Mean Reward: 961.29    Mean Loss: 1409.378    Time Delta: 134.914    Time: 2021-05-09T13:54:52\n",
      "Episode: 20520    Epsilon: 0.26127999999999996    Mean Reward: 957.71    Mean Loss: 1479.084    Time Delta: 135.061    Time: 2021-05-09T13:57:07\n",
      "Episode: 20560    Epsilon: 0.25983999999999996    Mean Reward: 960.81    Mean Loss: 1562.602    Time Delta: 137.695    Time: 2021-05-09T13:59:25\n",
      "Episode: 20600    Epsilon: 0.25839999999999996    Mean Reward: 917.0    Mean Loss: 1643.079    Time Delta: 124.43    Time: 2021-05-09T14:01:29\n",
      "Episode: 20640    Epsilon: 0.25695999999999997    Mean Reward: 896.45    Mean Loss: 1759.565    Time Delta: 129.821    Time: 2021-05-09T14:03:39\n",
      "Episode: 20680    Epsilon: 0.25551999999999997    Mean Reward: 924.3    Mean Loss: 1916.96    Time Delta: 147.591    Time: 2021-05-09T14:06:07\n",
      "Episode: 20720    Epsilon: 0.25408    Mean Reward: 849.44    Mean Loss: 1966.485    Time Delta: 115.236    Time: 2021-05-09T14:08:02\n"
     ]
    }
   ],
   "source": [
    "sv = Saver()\n",
    "dqn, replay_buffer = sv.deserialize(0)\n",
    "logger = MetricLogger(\"logger_surgery\")\n",
    "\n",
    "train(\n",
    "    dqn=dqn,\n",
    "    env=env,\n",
    "    replay_buffer=replay_buffer,\n",
    "    logger=logger,\n",
    "    saver=sv,\n",
    "    start_episode=dqn.step\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Mario DQN (refactored version, like general but ++).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
