# MarioDQN

Читать это на другом языке: [English](./README.md), [Русский](./README.ru.md)


## Цель проекта:
Создать нейронную сеть, которая использует алгоритм обучения с подкреплением (*Reinforcement Learning Algorithm*), чтобы научиться играть в Mario.


## Участники:
1) [Арсений Хлытчиев](https://github.com/arseniyx92)
2) [Егор Юхневич](https://github.com/Straple)
3) [Владислав Артюхов](https://github.com/Vladislav0Art)
4) [Дмитрий Артюхов](https://github.com/dmitrii-artuhov)
5) [Артем Брежнев](https://github.com/brezhart)

## Презентация проекта:
Мы использовали данную презентацию при выступлении на недельной смене по "Программированию и анализу данных" от [Высшей школы экономики](https://spb.hse.ru/).

[MarioDQN Презентация](https://docs.google.com/presentation/d/100cYpMxiK1RL7NthUdf9kef2058ZmRucG2jnOdevoAM/edit?usp=sharing)


# О проекте

## Алгоритм обучения с подкреплением — Reinforcement Learning:

![Работа алгоритма обучения с подкреплением](./assets/RL.png)

1)	Агент совершает в среде действие в зависимости от текущего состояния и получает награду 
2)	Среда переходит в следующее состояние 
3)	Агент опять совершает действие и получает награду
4)	Действия повторяются, пока агент не попадет в терминальное состояние (например, смерть в игре)

Основная цель агента заключается в максимизации суммы наград за весь эпизод — период от старта игры до терминального состояния.

Особенностью обучения с подкреплением является отсутсвие данных для тренировки, поэтому агент тренируется на данных, которые получает, взаимодействуя со средой.


## Что такое Q-learning:

Q-learning – это модель, которая обучает некоторую функцию полезности – Q-функцию. Эта функция на основании текущего состояния и конкретного действия вычисляет прогнозируемую награду за весь эпизод – Q value.

Агент совершает действия на основании своей политики – правила, которые определяют какое следующее действие выполнит агент. 
Политика нашего агента называется *Epsilon-Greedy*: с некоторой вероятностью агент совершает случайное действие, иначе он совершает действие, которое соответствует максимальному значению Q-функции:

$$$a_t = argmax_a Q(s_t, a)$$$

```
# realization of Epsilon-Greedy Policy:

def act(state):
    rand_float = random.random() # returns random float in range: [0, 1)
    if rand_float <= EPS:
        action = random_action()
    else:
        action = model.get_action(state) # returns action that brings max Q-value

    return action

```

В классической реализации алгоритма Q-learning формируется таблица из всех возможных состояний среды и всех возможных действий. Задача заключается в том, чтобы посчитать значения Q-values для каждой пары (состояние, действие).

![Таблица функции полезности вида: (состояние, действие)](./assets/Q-learning_table.png)


### Процесс обучения:

Мы добавляем к рассматриваемому значению Q-функции разность между оптимальным значением и текущим значением данной функции. 

1)  Q(s, a) – значение Q-функции для состояния и действия
2)  Q<sub>target</sub>(s, a) – это оптимальное, по нашему предположению, значение Q функции, к которому мы пытаемся свести текущее значение Q функции
3)  s<sub>t</sub>, a<sub>t</sub> – состояние среды и выбранное действие в момент времени $t$
4)  r<sub>t</sub>(s<sub>t</sub>, a<sub>t</sub>) – награда за текущее состояние среды и совершенное действие
5)  &gamma; – коэффициент дисконтирования. Он необходим для того, чтобы уменьшать "значимость" награды в последующих моментах времени
6)  &alpha; – коэффициент обучения. Он определяет насколько сильно мы изменим текущее значение Q-функции

![Q_{target}(s_t, a_t) = r_t(s_t, a_t) + \gamma \max_a Q(s_{t + 1}, a)](./assets/Q_target.svg)

![Q(s_t, a_t) := Q(s_t, a_t) + \alpha (Q_{target}(s_t, a_t) - Q(s_t, a_t))](./assets/Q_updating.svg)